{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85800008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EvalPrediction,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c37945cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 20000\n",
      "Validation examples: 5000\n",
      "Testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "# Load the IMDB dataset using Hugging Face datasets\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(20000))\n",
    "valid_dataset = dataset[\"train\"].shuffle(seed=42).select(range(20000, 25000))\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(valid_dataset)}\")\n",
    "print(f\"Testing examples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c71f86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2426f4e88e424653b81b7a0fb6568a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eeleoj62\\anaconda3\\envs\\ds-env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\eeleoj62\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af890761ba049d89451a233de45de0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd284c4c9d147139f77ef79c7311b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f95e02f7614a3c8d6f2c35d8028776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b31248327ee4c54b2e9aa478a2eb89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec5a3efbcc647bfab1cc67b0fc4eaea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5764ab9bf5bd422f9a2357ceaecb91fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_valid = valid_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3be621a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187d29afb56a4ec5863c4462f85cd833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "# Freeze BERT layers to prevent parameter updates (will save us some time)\n",
    "for param in model.bert.parameters():\n",
    "   param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701fae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics computation function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    use_cpu=True, # important for Mac users with M1+ chips\n",
    "    learning_rate=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa21f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "# Optimizer defaults to Adam with momentum\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=tokenizer,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)])\n",
    "\n",
    "# Train the model\n",
    "print('Fine-tuning BERT model...')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a62c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate(tokenized_test)\n",
    "print(f\"Test results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf58d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the test set\n",
    "def get_predictions(trainer, dataset):\n",
    "   # Run predictions with Hugging Face Trainer\n",
    "   raw_predictions = trainer.predict(dataset)\n",
    "  \n",
    "   # Extract predictions and labels\n",
    "   predictions = np.argmax(raw_predictions.predictions, axis=1)\n",
    "   labels = raw_predictions.label_ids\n",
    "  \n",
    "   return predictions, labels\n",
    "\n",
    "\n",
    "y_pred, y_true = get_predictions(trainer, tokenized_test)\n",
    "\n",
    "\n",
    "# Create and visualize the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix for BBC News Classification (Hugging Face)')\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create classification report\n",
    "report = classification_report(y_true, y_pred, target_names=['Negative', 'Positive'], output_dict=True)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c76061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions\n",
    "# Function to make predictions\n",
    "def predict_sentiment_hf(text):\n",
    "   # Handle both single texts and lists\n",
    "   if isinstance(text, str):\n",
    "       examples = [text]\n",
    "   else:\n",
    "       examples = text\n",
    "  \n",
    "   # Tokenize inputs\n",
    "   inputs = tokenizer(\n",
    "       examples,\n",
    "       padding=True,\n",
    "       truncation=True,\n",
    "       max_length=128,\n",
    "       return_tensors=\"pt\"\n",
    "   )\n",
    "  \n",
    "   # Get predictions\n",
    "   with torch.no_grad():\n",
    "       outputs = model(**inputs)\n",
    "       logits = outputs.logits\n",
    "       predictions = torch.softmax(logits, dim=1)\n",
    "  \n",
    "    # Convert to numpy for easier handling\n",
    "   probs = predictions[0].numpy()\n",
    "  \n",
    "   # Get the predicted category and confidence\n",
    "   predicted_class_id = np.argmax(probs)\n",
    "   predicted_sentiment = 'Postive' if predicted_class_id == 1 else 'Negative'\n",
    "   confidence = float(probs[predicted_class_id])\n",
    "  \n",
    "   return {\n",
    "       'text': text[:100] + '...' if len(text) > 100 else text,\n",
    "       'predicted_sentiment': predicted_sentiment,\n",
    "       'confidence': confidence}\n",
    "predict_sentiment_hf(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c508b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model_save_path = \"./bert_sentiment_model_hf\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Model and tokenizer saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e5f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_save_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
